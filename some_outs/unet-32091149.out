The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) blis/0.9.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.
wandb: Tracking run with wandb version 0.17.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
ARGS:  ['unet.py', '--dataset', 'new', '--batch_size', '16', '--gendice', '--mem_feat']
Data loaders created.
Current dataset new
Starting training...
SAVING MODELS TO /home/mishaalk/scratch/gapjunc/models/newgendice01234567
running for 300 epochs
Using device: cuda
  0%|          | 0/153 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/unet.py", line 751, in <module>
    train_wrapper(model, train_loader, criterion, optimizer, valid_loader, epochs=300 if not args.epochs else args.epochs, mem_feat=args.mem_feat, gen_gj_entities=args.customloss, fn_rwt=args.fn_rwt)
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/unet.py", line 165, in train_loop
    else: loss = criterion(pred.squeeze(1), labels.squeeze(1) if not gen_gj_entities else (label_centers, label_contours, pad_mask), neuron_mask.squeeze(1) if neuron_mask != [] and not mem_feat else [], mito_mask.squeeze(1) if mito_mask != [] else [], loss_fn = loss_f_, fn_reweight=fn_rwt)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: GenDLoss.forward() got an unexpected keyword argument 'fn_reweight'
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mishaalk/scratch/gapjunc/wandb/offline-run-20240726_223652-840hw0v4
wandb: Find logs at: /home/mishaalk/scratch/gapjunc/wandb/offline-run-20240726_223652-840hw0v4/logs
wandb: Tracking run with wandb version 0.17.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2450 /home/mishaalk/scratch/gapjunc/train_datasets/final_jnc_only_split_rwt/train_imgs /home/mishaalk/scratch/gapjunc/train_datasets/final_jnc_only_split_rwt/train_gts
245 /home/mishaalk/scratch/gapjunc/train_datasets/final_jnc_only_split_rwt/valid_imgs /home/mishaalk/scratch/gapjunc/train_datasets/final_jnc_only_split_rwt/valid_gts
ARGS:  ['unet.py', '--dataset', 'new', '--batch_size', '16', '--focal', '--fn_rwt', '--model_name', '2d_gd_mem_run1/model5_epoch71.pk1']
Data loaders created.
Current dataset new
Starting training...
SAVING MODELS TO /home/mishaalk/scratch/gapjunc/models/newfocal012345678910111213
running for 300 epochs
Using device: cuda
  0%|          | 0/154 [00:00<?, ?it/s]  0%|          | 0/154 [00:05<?, ?it/s, loss=Loss: -332073.90625, step=Step: 0]  1%|          | 1/154 [00:05<14:52,  5.83s/it, loss=Loss: -332073.90625, step=Step: 0]  1%|          | 1/154 [00:05<14:52,  5.83s/it, loss=Loss: -332073.90625, step=Step: 0]  1%|          | 1/154 [00:06<14:52,  5.83s/it, loss=Loss: -20758494.0, step=Step: 1]    1%|▏         | 2/154 [00:06<06:54,  2.73s/it, loss=Loss: -20758494.0, step=Step: 1]  1%|▏         | 2/154 [00:06<06:54,  2.73s/it, loss=Loss: -20758494.0, step=Step: 1]  1%|▏         | 2/154 [00:06<06:54,  2.73s/it, loss=Loss: -1.164388118782227e+28, step=Step: 2]  2%|▏         | 3/154 [00:06<04:20,  1.73s/it, loss=Loss: -1.164388118782227e+28, step=Step: 2]  2%|▏         | 3/154 [00:06<04:20,  1.73s/it, loss=Loss: -1.164388118782227e+28, step=Step: 2]  2%|▏         | 3/154 [00:07<04:20,  1.73s/it, loss=Loss: -35181514752.0, step=Step: 3]          3%|▎         | 4/154 [00:07<03:08,  1.26s/it, loss=Loss: -35181514752.0, step=Step: 3]  3%|▎         | 4/154 [00:07<03:08,  1.26s/it, loss=Loss: -35181514752.0, step=Step: 3]  3%|▎         | 4/154 [00:08<03:08,  1.26s/it, loss=Loss: -0.43294456601142883, step=Step: 4]  3%|▎         | 5/154 [00:08<02:28,  1.00it/s, loss=Loss: -0.43294456601142883, step=Step: 4]  3%|▎         | 5/154 [00:08<02:28,  1.00it/s, loss=Loss: -0.43294456601142883, step=Step: 4]  3%|▎         | 5/154 [00:08<02:28,  1.00it/s, loss=Loss: 0.05667082965373993, step=Step: 5]   4%|▍         | 6/154 [00:08<02:04,  1.19it/s, loss=Loss: 0.05667082965373993, step=Step: 5]  4%|▍         | 6/154 [00:08<02:04,  1.19it/s, loss=Loss: 0.05667082965373993, step=Step: 5]  4%|▍         | 6/154 [00:09<02:04,  1.19it/s, loss=Loss: -2760195328.0, step=Step: 6]        5%|▍         | 7/154 [00:09<01:49,  1.34it/s, loss=Loss: -2760195328.0, step=Step: 6]  5%|▍         | 7/154 [00:09<01:49,  1.34it/s, loss=Loss: -2760195328.0, step=Step: 6]  5%|▍         | 7/154 [00:09<01:49,  1.34it/s, loss=Loss: 0.09685130417346954, step=Step: 7]  5%|▌         | 8/154 [00:09<01:39,  1.47it/s, loss=Loss: 0.09685130417346954, step=Step: 7]  5%|▌         | 8/154 [00:09<01:39,  1.47it/s, loss=Loss: 0.09685130417346954, step=Step: 7]  5%|▌         | 8/154 [00:10<01:39,  1.47it/s, loss=Loss: -0.012271564453840256, step=Step: 8]  6%|▌         | 9/154 [00:10<01:32,  1.57it/s, loss=Loss: -0.012271564453840256, step=Step: 8]  6%|▌         | 9/154 [00:10<01:32,  1.57it/s, loss=Loss: -0.012271564453840256, step=Step: 8]  6%|▌         | 9/154 [00:10<01:32,  1.57it/s, loss=Loss: -4118.583984375, step=Step: 9]        6%|▋         | 10/154 [00:10<01:27,  1.65it/s, loss=Loss: -4118.583984375, step=Step: 9]  6%|▋         | 10/154 [00:10<01:27,  1.65it/s, loss=Loss: -4118.583984375, step=Step: 9]  6%|▋         | 10/154 [00:11<01:27,  1.65it/s, loss=Loss: 0.10084687173366547, step=Step: 10]  7%|▋         | 11/154 [00:11<01:23,  1.71it/s, loss=Loss: 0.10084687173366547, step=Step: 10]  7%|▋         | 11/154 [00:11<01:23,  1.71it/s, loss=Loss: 0.10084687173366547, step=Step: 10]  7%|▋         | 11/154 [00:11<01:23,  1.71it/s, loss=Loss: 0.07088664919137955, step=Step: 11]  8%|▊         | 12/154 [00:11<01:21,  1.75it/s, loss=Loss: 0.07088664919137955, step=Step: 11]  8%|▊         | 12/154 [00:11<01:21,  1.75it/s, loss=Loss: 0.07088664919137955, step=Step: 11]  8%|▊         | 12/154 [00:12<01:21,  1.75it/s, loss=Loss: -4.273015975952148, step=Step: 12]   8%|▊         | 13/154 [00:12<01:19,  1.77it/s, loss=Loss: -4.273015975952148, step=Step: 12]  8%|▊         | 13/154 [00:12<01:19,  1.77it/s, loss=Loss: -4.273015975952148, step=Step: 12]  8%|▊         | 13/154 [00:12<01:19,  1.77it/s, loss=Loss: -3222255.25, step=Step: 13]         9%|▉         | 14/154 [00:12<01:18,  1.79it/s, loss=Loss: -3222255.25, step=Step: 13]  9%|▉         | 14/154 [00:12<01:18,  1.79it/s, loss=Loss: -3222255.25, step=Step: 13]  9%|▉         | 14/154 [00:13<01:18,  1.79it/s, loss=Loss: -25404343582720.0, step=Step: 14] 10%|▉         | 15/154 [00:13<01:16,  1.81it/s, loss=Loss: -25404343582720.0, step=Step: 14] 10%|▉         | 15/154 [00:13<01:16,  1.81it/s, loss=Loss: -25404343582720.0, step=Step: 14] 10%|▉         | 15/154 [00:13<01:16,  1.81it/s, loss=Loss: 0.0557890310883522, step=Step: 15] 10%|█         | 16/154 [00:13<01:15,  1.82it/s, loss=Loss: 0.0557890310883522, step=Step: 15] 10%|█         | 16/154 [00:13<01:15,  1.82it/s, loss=Loss: 0.0557890310883522, step=Step: 15] 10%|█         | 16/154 [00:14<01:15,  1.82it/s, loss=Loss: -3.4874107837677, step=Step: 16]   11%|█         | 17/154 [00:14<01:14,  1.83it/s, loss=Loss: -3.4874107837677, step=Step: 16] 11%|█         | 17/154 [00:14<01:14,  1.83it/s, loss=Loss: -3.4874107837677, step=Step: 16] 11%|█         | 17/154 [00:15<01:14,  1.83it/s, loss=Loss: -2.9451420439859813e+31, step=Step: 17] 12%|█▏        | 18/154 [00:15<01:14,  1.83it/s, loss=Loss: -2.9451420439859813e+31, step=Step: 17] 12%|█▏        | 18/154 [00:15<01:14,  1.83it/s, loss=Loss: -2.9451420439859813e+31, step=Step: 17] 12%|█▏        | 18/154 [00:15<01:14,  1.83it/s, loss=Loss: 0.13891366124153137, step=Step: 18]     12%|█▏        | 19/154 [00:15<01:13,  1.84it/s, loss=Loss: 0.13891366124153137, step=Step: 18] 12%|█▏        | 19/154 [00:15<01:13,  1.84it/s, loss=Loss: 0.13891366124153137, step=Step: 18] 12%|█▏        | 19/154 [00:16<01:13,  1.84it/s, loss=Loss: -813991127416832.0, step=Step: 19]  13%|█▎        | 20/154 [00:16<01:12,  1.84it/s, loss=Loss: -813991127416832.0, step=Step: 19] 13%|█▎        | 20/154 [00:16<01:12,  1.84it/s, loss=Loss: -813991127416832.0, step=Step: 19] 13%|█▎        | 20/154 [00:16<01:12,  1.84it/s, loss=Loss: 0.07685431838035583, step=Step: 20] 14%|█▎        | 21/154 [00:16<01:12,  1.85it/s, loss=Loss: 0.07685431838035583, step=Step: 20] 14%|█▎        | 21/154 [00:16<01:12,  1.85it/s, loss=Loss: 0.07685431838035583, step=Step: 20] 14%|█▎        | 21/154 [00:17<01:12,  1.85it/s, loss=Loss: -9.665455975569555e+18, step=Step: 21] 14%|█▍        | 22/154 [00:17<01:11,  1.85it/s, loss=Loss: -9.665455975569555e+18, step=Step: 21] 14%|█▍        | 22/154 [00:17<01:11,  1.85it/s, loss=Loss: -9.665455975569555e+18, step=Step: 21] 14%|█▍        | 22/154 [00:17<01:11,  1.85it/s, loss=Loss: -5856384516096.0, step=Step: 22]       15%|█▍        | 23/154 [00:17<01:10,  1.85it/s, loss=Loss: -5856384516096.0, step=Step: 22] 15%|█▍        | 23/154 [00:17<01:10,  1.85it/s, loss=Loss: -5856384516096.0, step=Step: 22] 15%|█▍        | 23/154 [00:18<01:10,  1.85it/s, loss=Loss: 0.099151611328125, step=Step: 23] 16%|█▌        | 24/154 [00:18<01:10,  1.85it/s, loss=Loss: 0.099151611328125, step=Step: 23] 16%|█▌        | 24/154 [00:18<01:10,  1.85it/s, loss=Loss: 0.099151611328125, step=Step: 23] 16%|█▌        | 24/154 [00:18<01:10,  1.85it/s, loss=Loss: -0.008334603160619736, step=Step: 24] 16%|█▌        | 25/154 [00:18<01:09,  1.85it/s, loss=Loss: -0.008334603160619736, step=Step: 24] 16%|█▌        | 25/154 [00:18<01:09,  1.85it/s, loss=Loss: -0.008334603160619736, step=Step: 24] 16%|█▌        | 25/154 [00:19<01:09,  1.85it/s, loss=Loss: -0.0973668098449707, step=Step: 25]   17%|█▋        | 26/154 [00:19<01:09,  1.84it/s, loss=Loss: -0.0973668098449707, step=Step: 25] 17%|█▋        | 26/154 [00:19<01:09,  1.84it/s, loss=Loss: -0.0973668098449707, step=Step: 25] 17%|█▋        | 26/154 [00:19<01:09,  1.84it/s, loss=Loss: 0.05791543796658516, step=Step: 26] 18%|█▊        | 27/154 [00:19<01:08,  1.84it/s, loss=Loss: 0.05791543796658516, step=Step: 26] 18%|█▊        | 27/154 [00:19<01:08,  1.84it/s, loss=Loss: 0.05791543796658516, step=Step: 26] 18%|█▊        | 27/154 [00:20<01:08,  1.84it/s, loss=Loss: -7074923008.0, step=Step: 27]       18%|█▊        | 28/154 [00:20<01:08,  1.84it/s, loss=Loss: -7074923008.0, step=Step: 27] 18%|█▊        | 28/154 [00:20<01:08,  1.84it/s, loss=Loss: -7074923008.0, step=Step: 27] 18%|█▊        | 28/154 [00:20<01:08,  1.84it/s, loss=Loss: 0.09258972853422165, step=Step: 28] 19%|█▉        | 29/154 [00:20<01:07,  1.85it/s, loss=Loss: 0.09258972853422165, step=Step: 28] 19%|█▉        | 29/154 [00:20<01:07,  1.85it/s, loss=Loss: 0.09258972853422165, step=Step: 28] 19%|█▉        | 29/154 [00:21<01:07,  1.85it/s, loss=Loss: -238.61370849609375, step=Step: 29] 19%|█▉        | 30/154 [00:21<01:07,  1.84it/s, loss=Loss: -238.61370849609375, step=Step: 29] 19%|█▉        | 30/154 [00:21<01:07,  1.84it/s, loss=Loss: -238.61370849609375, step=Step: 29] 19%|█▉        | 30/154 [00:22<01:07,  1.84it/s, loss=Loss: 0.05896372348070145, step=Step: 30] 20%|██        | 31/154 [00:22<01:06,  1.84it/s, loss=Loss: 0.05896372348070145, step=Step: 30] 20%|██        | 31/154 [00:22<01:06,  1.84it/s, loss=Loss: 0.05896372348070145, step=Step: 30] 20%|██        | 31/154 [00:22<01:06,  1.84it/s, loss=Loss: -3.6783647485560095e+18, step=Step: 31] 21%|██        | 32/154 [00:22<01:06,  1.85it/s, loss=Loss: -3.6783647485560095e+18, step=Step: 31] 21%|██        | 32/154 [00:22<01:06,  1.85it/s, loss=Loss: -3.6783647485560095e+18, step=Step: 31] 21%|██        | 32/154 [00:23<01:06,  1.85it/s, loss=Loss: -6472171008.0, step=Step: 32]           21%|██▏       | 33/154 [00:23<01:05,  1.85it/s, loss=Loss: -6472171008.0, step=Step: 32] 21%|██▏       | 33/154 [00:23<01:05,  1.85it/s, loss=Loss: -6472171008.0, step=Step: 32] 21%|██▏       | 33/154 [00:23<01:05,  1.85it/s, loss=Loss: -6.160492897033691, step=Step: 33] 22%|██▏       | 34/154 [00:23<01:04,  1.85it/s, loss=Loss: -6.160492897033691, step=Step: 33] 22%|██▏       | 34/154 [00:23<01:04,  1.85it/s, loss=Loss: -6.160492897033691, step=Step: 33] 22%|██▏       | 34/154 [00:24<01:04,  1.85it/s, loss=Loss: 0.04270124062895775, step=Step: 34] 23%|██▎       | 35/154 [00:24<01:04,  1.85it/s, loss=Loss: 0.04270124062895775, step=Step: 34] 23%|██▎       | 35/154 [00:24<01:04,  1.85it/s, loss=Loss: 0.04270124062895775, step=Step: 34] 23%|██▎       | 35/154 [00:24<01:04,  1.85it/s, loss=Loss: 0.07020195573568344, step=Step: 35] 23%|██▎       | 36/154 [00:24<01:03,  1.85it/s, loss=Loss: 0.07020195573568344, step=Step: 35] 23%|██▎       | 36/154 [00:24<01:03,  1.85it/s, loss=Loss: 0.07020195573568344, step=Step: 35] 23%|██▎       | 36/154 [00:25<01:03,  1.85it/s, loss=Loss: -2.424894279978582e+18, step=Step: 36] 24%|██▍       | 37/154 [00:25<01:03,  1.85it/s, loss=Loss: -2.424894279978582e+18, step=Step: 36] 24%|██▍       | 37/154 [00:25<01:03,  1.85it/s, loss=Loss: -2.424894279978582e+18, step=Step: 36] 24%|██▍       | 37/154 [00:25<01:03,  1.85it/s, loss=Loss: -130851.4375, step=Step: 37]           25%|██▍       | 38/154 [00:25<01:02,  1.85it/s, loss=Loss: -130851.4375, step=Step: 37] 25%|██▍       | 38/154 [00:25<01:02,  1.85it/s, loss=Loss: -130851.4375, step=Step: 37] 25%|██▍       | 38/154 [00:26<01:02,  1.85it/s, loss=Loss: 0.0752079114317894, step=Step: 38] 25%|██▌       | 39/154 [00:26<01:02,  1.85it/s, loss=Loss: 0.0752079114317894, step=Step: 38] 25%|██▌       | 39/154 [00:26<01:02,  1.85it/s, loss=Loss: 0.0752079114317894, step=Step: 38] 25%|██▌       | 39/154 [00:26<01:02,  1.85it/s, loss=Loss: -146266.671875, step=Step: 39]     26%|██▌       | 40/154 [00:26<01:01,  1.85it/s, loss=Loss: -146266.671875, step=Step: 39] 26%|██▌       | 40/154 [00:26<01:01,  1.85it/s, loss=Loss: -146266.671875, step=Step: 39] 26%|██▌       | 40/154 [00:27<01:01,  1.85it/s, loss=Loss: -115.71693420410156, step=Step: 40] 27%|██▋       | 41/154 [00:27<01:01,  1.85it/s, loss=Loss: -115.71693420410156, step=Step: 40] 27%|██▋       | 41/154 [00:27<01:01,  1.85it/s, loss=Loss: -115.71693420410156, step=Step: 40] 27%|██▋       | 41/154 [00:28<01:01,  1.85it/s, loss=Loss: -42.13762664794922, step=Step: 41]  27%|██▋       | 42/154 [00:28<01:00,  1.85it/s, loss=Loss: -42.13762664794922, step=Step: 41] 27%|██▋       | 42/154 [00:28<01:00,  1.85it/s, loss=Loss: -42.13762664794922, step=Step: 41] 27%|██▋       | 42/154 [00:28<01:00,  1.85it/s, loss=Loss: -175851680.0, step=Step: 42]       28%|██▊       | 43/154 [00:28<01:00,  1.85it/s, loss=Loss: -175851680.0, step=Step: 42] 28%|██▊       | 43/154 [00:28<01:00,  1.85it/s, loss=Loss: -175851680.0, step=Step: 42] 28%|██▊       | 43/154 [00:29<01:00,  1.85it/s, loss=Loss: 0.07030780613422394, step=Step: 43] 29%|██▊       | 44/154 [00:29<00:59,  1.85it/s, loss=Loss: 0.07030780613422394, step=Step: 43] 29%|██▊       | 44/154 [00:29<00:59,  1.85it/s, loss=Loss: 0.07030780613422394, step=Step: 43] 29%|██▊       | 44/154 [00:29<00:59,  1.85it/s, loss=Loss: 0.05951152741909027, step=Step: 44] 29%|██▉       | 45/154 [00:29<00:59,  1.85it/s, loss=Loss: 0.05951152741909027, step=Step: 44] 29%|██▉       | 45/154 [00:29<00:59,  1.85it/s, loss=Loss: 0.05951152741909027, step=Step: 44] 29%|██▉       | 45/154 [00:30<00:59,  1.85it/s, loss=Loss: -75.65489196777344, step=Step: 45]  30%|██▉       | 46/154 [00:30<00:58,  1.84it/s, loss=Loss: -75.65489196777344, step=Step: 45] 30%|██▉       | 46/154 [00:30<00:58,  1.84it/s, loss=Loss: -75.65489196777344, step=Step: 45] 30%|██▉       | 46/154 [00:30<00:58,  1.84it/s, loss=Loss: -651184000.0, step=Step: 46]       31%|███       | 47/154 [00:30<00:58,  1.84it/s, loss=Loss: -651184000.0, step=Step: 46] 31%|███       | 47/154 [00:30<00:58,  1.84it/s, loss=Loss: -651184000.0, step=Step: 46] 31%|███       | 47/154 [00:31<00:58,  1.84it/s, loss=Loss: 0.25501468777656555, step=Step: 47] 31%|███       | 48/154 [00:31<00:57,  1.84it/s, loss=Loss: 0.25501468777656555, step=Step: 47] 31%|███       | 48/154 [00:31<00:57,  1.84it/s, loss=Loss: 0.25501468777656555, step=Step: 47] 31%|███       | 48/154 [00:31<00:57,  1.84it/s, loss=Loss: -584175714304.0, step=Step: 48]     32%|███▏      | 49/154 [00:31<00:56,  1.84it/s, loss=Loss: -584175714304.0, step=Step: 48] 32%|███▏      | 49/154 [00:31<00:56,  1.84it/s, loss=Loss: -584175714304.0, step=Step: 48] 32%|███▏      | 49/154 [00:32<00:56,  1.84it/s, loss=Loss: 0.08276961743831635, step=Step: 49] 32%|███▏      | 50/154 [00:32<00:56,  1.84it/s, loss=Loss: 0.08276961743831635, step=Step: 49] 32%|███▏      | 50/154 [00:32<00:56,  1.84it/s, loss=Loss: 0.08276961743831635, step=Step: 49] 32%|███▏      | 50/154 [00:32<00:56,  1.84it/s, loss=Loss: -216709.5625, step=Step: 50]        33%|███▎      | 51/154 [00:32<00:55,  1.84it/s, loss=Loss: -216709.5625, step=Step: 50] 33%|███▎      | 51/154 [00:32<00:55,  1.84it/s, loss=Loss: -216709.5625, step=Step: 50] 33%|███▎      | 51/154 [00:33<00:55,  1.84it/s, loss=Loss: -0.3348057270050049, step=Step: 51] 34%|███▍      | 52/154 [00:33<00:55,  1.84it/s, loss=Loss: -0.3348057270050049, step=Step: 51] 34%|███▍      | 52/154 [00:33<00:55,  1.84it/s, loss=Loss: -0.3348057270050049, step=Step: 51] 34%|███▍      | 52/154 [00:33<00:55,  1.84it/s, loss=Loss: -202849.59375, step=Step: 52]       34%|███▍      | 53/154 [00:33<00:54,  1.84it/s, loss=Loss: -202849.59375, step=Step: 52] 34%|███▍      | 53/154 [00:33<00:54,  1.84it/s, loss=Loss: -202849.59375, step=Step: 52] 34%|███▍      | 53/154 [00:34<00:54,  1.84it/s, loss=Loss: -6.603725433349609, step=Step: 53] 35%|███▌      | 54/154 [00:34<00:54,  1.84it/s, loss=Loss: -6.603725433349609, step=Step: 53] 35%|███▌      | 54/154 [00:34<00:54,  1.84it/s, loss=Loss: -6.603725433349609, step=Step: 53] 35%|███▌      | 54/154 [00:35<00:54,  1.84it/s, loss=Loss: -241617.328125, step=Step: 54]     36%|███▌      | 55/154 [00:35<00:53,  1.84it/s, loss=Loss: -241617.328125, step=Step: 54] 36%|███▌      | 55/154 [00:35<00:53,  1.84it/s, loss=Loss: -241617.328125, step=Step: 54] 36%|███▌      | 55/154 [00:35<00:53,  1.84it/s, loss=Loss: 0.07688822597265244, step=Step: 55] 36%|███▋      | 56/154 [00:35<00:53,  1.84it/s, loss=Loss: 0.07688822597265244, step=Step: 55] 36%|███▋      | 56/154 [00:35<00:53,  1.84it/s, loss=Loss: 0.07688822597265244, step=Step: 55] 36%|███▋      | 56/154 [00:36<00:53,  1.84it/s, loss=Loss: -1232359784448.0, step=Step: 56]    37%|███▋      | 57/154 [00:36<00:52,  1.84it/s, loss=Loss: -1232359784448.0, step=Step: 56] 37%|███▋      | 57/154 [00:36<00:52,  1.84it/s, loss=Loss: -1232359784448.0, step=Step: 56] 37%|███▋      | 57/154 [00:36<00:52,  1.84it/s, loss=Loss: -248.10252380371094, step=Step: 57] 38%|███▊      | 58/154 [00:36<00:52,  1.84it/s, loss=Loss: -248.10252380371094, step=Step: 57] 38%|███▊      | 58/154 [00:36<00:52,  1.84it/s, loss=Loss: -248.10252380371094, step=Step: 57] 38%|███▊      | 58/154 [00:37<00:52,  1.84it/s, loss=Loss: 0.12535518407821655, step=Step: 58] 38%|███▊      | 59/154 [00:37<00:51,  1.84it/s, loss=Loss: 0.12535518407821655, step=Step: 58] 38%|███▊      | 59/154 [00:37<00:51,  1.84it/s, loss=Loss: 0.12535518407821655, step=Step: 58] 38%|███▊      | 59/154 [00:37<00:51,  1.84it/s, loss=Loss: -328632384.0, step=Step: 59]        39%|███▉      | 60/154 [00:37<00:50,  1.84it/s, loss=Loss: -328632384.0, step=Step: 59] 39%|███▉      | 60/154 [00:37<00:50,  1.84it/s, loss=Loss: -328632384.0, step=Step: 59] 39%|███▉      | 60/154 [00:38<00:50,  1.84it/s, loss=Loss: -964.744140625, step=Step: 60] 40%|███▉      | 61/154 [00:38<00:50,  1.84it/s, loss=Loss: -964.744140625, step=Step: 60] 40%|███▉      | 61/154 [00:38<00:50,  1.84it/s, loss=Loss: -964.744140625, step=Step: 60] 40%|███▉      | 61/154 [00:38<00:50,  1.84it/s, loss=Loss: 0.06345762312412262, step=Step: 61] 40%|████      | 62/154 [00:38<00:49,  1.84it/s, loss=Loss: 0.06345762312412262, step=Step: 61] 40%|████      | 62/154 [00:38<00:49,  1.84it/s, loss=Loss: 0.06345762312412262, step=Step: 61] 40%|████      | 62/154 [00:39<00:49,  1.84it/s, loss=Loss: -2229779456.0, step=Step: 62]       41%|████      | 63/154 [00:39<00:49,  1.84it/s, loss=Loss: -2229779456.0, step=Step: 62] 41%|████      | 63/154 [00:39<00:49,  1.84it/s, loss=Loss: -2229779456.0, step=Step: 62] 41%|████      | 63/154 [00:39<00:49,  1.84it/s, loss=Loss: -77248552.0, step=Step: 63]   42%|████▏     | 64/154 [00:39<00:48,  1.84it/s, loss=Loss: -77248552.0, step=Step: 63] 42%|████▏     | 64/154 [00:39<00:48,  1.84it/s, loss=Loss: -77248552.0, step=Step: 63] 42%|████▏     | 64/154 [00:40<00:48,  1.84it/s, loss=Loss: -80704.7578125, step=Step: 64] 42%|████▏     | 65/154 [00:40<00:48,  1.84it/s, loss=Loss: -80704.7578125, step=Step: 64] 42%|████▏     | 65/154 [00:40<00:48,  1.84it/s, loss=Loss: -80704.7578125, step=Step: 64] 42%|████▏     | 65/154 [00:41<00:48,  1.84it/s, loss=Loss: 0.09473197162151337, step=Step: 65] 43%|████▎     | 66/154 [00:41<00:47,  1.84it/s, loss=Loss: 0.09473197162151337, step=Step: 65] 43%|████▎     | 66/154 [00:41<00:47,  1.84it/s, loss=Loss: 0.09473197162151337, step=Step: 65] 43%|████▎     | 66/154 [00:41<00:47,  1.84it/s, loss=Loss: 0.22551849484443665, step=Step: 66] 44%|████▎     | 67/154 [00:41<00:47,  1.84it/s, loss=Loss: 0.22551849484443665, step=Step: 66] 44%|████▎     | 67/154 [00:41<00:47,  1.84it/s, loss=Loss: 0.22551849484443665, step=Step: 66] 44%|████▎     | 67/154 [00:42<00:47,  1.84it/s, loss=Loss: -83.5085220336914, step=Step: 67]   44%|████▍     | 68/154 [00:42<00:46,  1.84it/s, loss=Loss: -83.5085220336914, step=Step: 67] 44%|████▍     | 68/154 [00:42<00:46,  1.84it/s, loss=Loss: -83.5085220336914, step=Step: 67] 44%|████▍     | 68/154 [00:42<00:46,  1.84it/s, loss=Loss: -743.7459106445312, step=Step: 68] 45%|████▍     | 69/154 [00:42<00:46,  1.84it/s, loss=Loss: -743.7459106445312, step=Step: 68] 45%|████▍     | 69/154 [00:42<00:46,  1.84it/s, loss=Loss: -743.7459106445312, step=Step: 68] 45%|████▍     | 69/154 [00:43<00:46,  1.84it/s, loss=Loss: -292802.90625, step=Step: 69]      45%|████▌     | 70/154 [00:43<00:45,  1.84it/s, loss=Loss: -292802.90625, step=Step: 69] 45%|████▌     | 70/154 [00:43<00:45,  1.84it/s, loss=Loss: -292802.90625, step=Step: 69] 45%|████▌     | 70/154 [00:43<00:45,  1.84it/s, loss=Loss: 0.061964862048625946, step=Step: 70] 46%|████▌     | 71/154 [00:43<00:45,  1.84it/s, loss=Loss: 0.061964862048625946, step=Step: 70] 46%|████▌     | 71/154 [00:43<00:45,  1.84it/s, loss=Loss: 0.061964862048625946, step=Step: 70] 46%|████▌     | 71/154 [00:44<00:45,  1.84it/s, loss=Loss: -4.4231065097555264e+21, step=Step: 71] 47%|████▋     | 72/154 [00:44<00:44,  1.84it/s, loss=Loss: -4.4231065097555264e+21, step=Step: 71] 47%|████▋     | 72/154 [00:44<00:44,  1.84it/s, loss=Loss: -4.4231065097555264e+21, step=Step: 71] 47%|████▋     | 72/154 [00:44<00:44,  1.84it/s, loss=Loss: 0.07380931824445724, step=Step: 72]     47%|████▋     | 73/154 [00:44<00:44,  1.84it/s, loss=Loss: 0.07380931824445724, step=Step: 72] 47%|████▋     | 73/154 [00:44<00:44,  1.84it/s, loss=Loss: 0.07380931824445724, step=Step: 72] 47%|████▋     | 73/154 [00:45<00:44,  1.84it/s, loss=Loss: 0.05839207023382187, step=Step: 73] 48%|████▊     | 74/154 [00:45<00:43,  1.84it/s, loss=Loss: 0.05839207023382187, step=Step: 73] 48%|████▊     | 74/154 [00:45<00:43,  1.84it/s, loss=Loss: 0.05839207023382187, step=Step: 73] 48%|████▊     | 74/154 [00:45<00:43,  1.84it/s, loss=Loss: -207537.84375, step=Step: 74]       49%|████▊     | 75/154 [00:45<00:42,  1.84it/s, loss=Loss: -207537.84375, step=Step: 74] 49%|████▊     | 75/154 [00:45<00:42,  1.84it/s, loss=Loss: -207537.84375, step=Step: 74] 49%|████▊     | 75/154 [00:46<00:42,  1.84it/s, loss=Loss: -6941351.0, step=Step: 75]    49%|████▉     | 76/154 [00:46<00:42,  1.84it/s, loss=Loss: -6941351.0, step=Step: 75] 49%|████▉     | 76/154 [00:46<00:42,  1.84it/s, loss=Loss: -6941351.0, step=Step: 75] 49%|████▉     | 76/154 [00:47<00:42,  1.84it/s, loss=Loss: -1594.663330078125, step=Step: 76] 50%|█████     | 77/154 [00:47<00:41,  1.84it/s, loss=Loss: -1594.663330078125, step=Step: 76] 50%|█████     | 77/154 [00:47<00:41,  1.84it/s, loss=Loss: -1594.663330078125, step=Step: 76] 50%|█████     | 77/154 [00:47<00:41,  1.84it/s, loss=Loss: -2472006909952.0, step=Step: 77]   51%|█████     | 78/154 [00:47<00:41,  1.84it/s, loss=Loss: -2472006909952.0, step=Step: 77] 51%|█████     | 78/154 [00:47<00:41,  1.84it/s, loss=Loss: -2472006909952.0, step=Step: 77] 51%|█████     | 78/154 [00:48<00:41,  1.84it/s, loss=Loss: -1.5979648888189485e+19, step=Step: 78] 51%|█████▏    | 79/154 [00:48<00:40,  1.84it/s, loss=Loss: -1.5979648888189485e+19, step=Step: 78] 51%|█████▏    | 79/154 [00:48<00:40,  1.84it/s, loss=Loss: -1.5979648888189485e+19, step=Step: 78]/home/mishaalk/py10/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Error detected in ConvolutionBackward0. Traceback of forward call that caused the error:
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/unet.py", line 751, in <module>
    train_wrapper(model, train_loader, criterion, optimizer, valid_loader, epochs=300 if not args.epochs else args.epochs, mem_feat=args.mem_feat, gen_gj_entities=args.customloss, fn_rwt=args.fn_rwt)
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/unet.py", line 149, in train_loop
    pred = model(inputs) if not mem_feat else model(inputs, neuron_mask.to(torch.float32))
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/utilities.py", line 366, in forward
    x = self.conv_last(x) # x: (16, 1, 512, 512)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
 (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.10/torch/torch/csrc/autograd/python_anomaly_mode.cpp:111.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/unet.py", line 751, in <module>
    train_wrapper(model, train_loader, criterion, optimizer, valid_loader, epochs=300 if not args.epochs else args.epochs, mem_feat=args.mem_feat, gen_gj_entities=args.customloss, fn_rwt=args.fn_rwt)
  File "/lustre06/project/6002514/mishaalk/gapjncsegmentation/unet.py", line 167, in train_loop
    loss.backward() # calculate gradients (backpropagation)
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/mishaalk/py10/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'ConvolutionBackward0' returned nan values in its 1th output.
wandb: 
wandb: Run history:
wandb:            loss █▁██████████████████████████████████████
wandb: train_precision █▂▂▄▂▆▄▃▄▆▃▂▇▇▇▆▃▆▄▄▆▅▅▄▂▄▅▃▄▇▆▅▇▇▇▅▅▄▁▇
wandb:    train_recall ▅▅▄▂▇▅▄▆▆▄▆█▇▇▆▇▅▅▆▆▆▇▆█▇▆▅▇▅▄▆▇▅▁▆▇▆▇█▇
wandb: 
wandb: Run summary:
wandb:            loss -1.5979648888189485e+19
wandb: train_precision 0.71296
wandb:    train_recall 0.85566
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mishaalk/scratch/gapjunc/wandb/offline-run-20240726_223714-lvjul56w
wandb: Find logs at: /home/mishaalk/scratch/gapjunc/wandb/offline-run-20240726_223714-lvjul56w/logs
