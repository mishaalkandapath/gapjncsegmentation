{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huayinluo/Documents/code/gapjncsegmentation/gapvenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchio as tio\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "from models import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_y_x(file_name, pattern):\n",
    "    \"\"\" get z, y, x from file name (uses basename of file_name)\"\"\"\n",
    "    file_name = os.path.basename(file_name)\n",
    "    match = re.match(pattern, file_name)\n",
    "    if match:\n",
    "        if len(match.groups()) != 3:\n",
    "            return None\n",
    "        z, y, x = match.groups()\n",
    "        return int(z), int(y), int(x)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dir=\"/Volumes/LaCie/zhenlab/gjsegmentation/gj_test_datasets/100_110_3x512x512/ground_truth\"\n",
    "img_dir=\"/Volumes/LaCie/zhenlab/gjsegmentation/gj_test_datasets/100_110_3x512x512/original\"\n",
    "save_dir=\"/Volumes/LaCie/filtered_100_110_3x512x512\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(os.path.join(save_dir, \"original\", \"train\")):\n",
    "    os.makedirs(os.path.join(save_dir, \"original\", \"train\"), exist_ok=True)\n",
    "if not os.path.exists(os.path.join(save_dir, \"original\", \"valid\")):\n",
    "    os.makedirs(os.path.join(save_dir, \"original\", \"valid\"), exist_ok=True)\n",
    "if not os.path.exists(os.path.join(save_dir, \"ground_truth\", \"train\")):\n",
    "    os.makedirs(os.path.join(save_dir, \"ground_truth\", \"train\"), exist_ok=True)\n",
    "if not os.path.exists(os.path.join(save_dir, \"ground_truth\", \"valid\")):\n",
    "    os.makedirs(os.path.join(save_dir, \"ground_truth\", \"valid\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271\n",
      "['z0_y0_x0.npy', 'z0_y0_x1024.npy', 'z0_y0_x1536.npy', 'z0_y0_x2048.npy', 'z0_y0_x2560.npy', 'z0_y0_x3072.npy', 'z0_y0_x3584.npy', 'z0_y0_x4096.npy', 'z0_y0_x4608.npy', 'z0_y0_x512.npy']\n",
      "['z0_y0_x0.npy', 'z0_y0_x1024.npy', 'z0_y0_x1536.npy', 'z0_y0_x2048.npy', 'z0_y0_x2560.npy', 'z0_y0_x3072.npy', 'z0_y0_x3584.npy', 'z0_y0_x4096.npy', 'z0_y0_x4608.npy', 'z0_y0_x512.npy']\n"
     ]
    }
   ],
   "source": [
    "img_paths = os.listdir(img_dir)\n",
    "gt_paths = os.listdir(gt_dir)\n",
    "print(len(img_paths))\n",
    "print(img_paths[:10])\n",
    "print(gt_paths[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved z9_y8192_x9216 | 1267/1271 | num saved 316\r"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "imgs=[]\n",
    "gts=[]\n",
    "num_saved = 0\n",
    "for i in range(len(img_paths)):\n",
    "    img_fp = img_paths[i]\n",
    "    gt_fp = gt_paths[i]\n",
    "    try:\n",
    "        img = np.load(os.path.join(img_dir, img_fp))\n",
    "        gt = np.load(os.path.join(gt_dir, gt_fp))\n",
    "    except:\n",
    "        continue\n",
    "    suffix = os.path.splitext(img_fp)[0]\n",
    "    print(f\"Saved {suffix} | {i}/{len(img_paths)} | num saved {num_saved}\", end=\"\\r\")\n",
    "    depth = gt.shape[0]\n",
    "    has_empty = False\n",
    "    for k in range(depth):\n",
    "        if len(np.unique(gt[k])) < 2:\n",
    "            has_empty = True\n",
    "    if not has_empty:\n",
    "        np.save(os.path.join(save_dir, \"original\", \"train\", f\"{suffix}.npy\"), img)\n",
    "        np.save(os.path.join(save_dir, \"ground_truth\", \"train\", f\"{suffix}.npy\"), gt)\n",
    "        num_saved += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/LaCie/filtered_100_110_3x512x512/original/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img_paths \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m gt_paths \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(img_paths))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/LaCie/filtered_100_110_3x512x512/original/train'"
     ]
    }
   ],
   "source": [
    "img_paths = os.listdir(os.path.join(save_dir, \"original\", \"train\"))\n",
    "gt_paths = os.listdir(os.path.join(save_dir, \"ground_truth\", \"train\"))\n",
    "print(len(img_paths))\n",
    "print(img_paths[:10])\n",
    "print(gt_paths[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -- args\n",
    "start_x = \n",
    "start_y = \n",
    "start_z = \n",
    "ending_depth = \n",
    "ending_height = \n",
    "ending_width = \n",
    "subvol_depth = \n",
    "subvol_height = \n",
    "subvol_width = \n",
    "mask_dir= \n",
    "img_dir= \n",
    "save_dir=\n",
    "if not os.path.exists(os.path.join(save_dir, \"visualize\")): \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"original\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"ground_truth\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"pred\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"visualize\"), exist_ok=True)\n",
    "    \n",
    "# -- get imgs & mask fp\n",
    "img_files = os.listdir(img_dir)\n",
    "mask_files = os.listdir(mask_dir)\n",
    "mask_pattern=r\"sem_dauer_2_gj_gt_s(\\d+).png\"\n",
    "img_pattern=r\"SEM_dauer_2_em_s(\\d+).png\"\n",
    "img_files = [os.path.join(img_dir, f) for f in img_files if f.endswith(\".png\")]\n",
    "mask_files = [os.path.join(mask_dir, f) for f in mask_files if f.endswith(\".png\")]\n",
    "\n",
    "# -- load model\n",
    "fp=\n",
    "model = UNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model, optimizer, epoch, loss, batch_size, lr, focal_loss_weights = load_checkpoint(model, optimizer, fp)\n",
    "model = model.eval()\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- get masks:\n",
    "i=0\n",
    "for z in range(start_z, ending_depth):\n",
    "    tmp_img = get_img_by_z(z, img_files, img_pattern)\n",
    "    tmp_mask = get_img_by_z(z, mask_files, mask_pattern)\n",
    "    if i == 0:\n",
    "        h, w = tmp_img.shape[0], tmp_img.shape[1]\n",
    "        full_volume_img = np.zeros((ending_depth-start_z, h, w))\n",
    "        full_volume_mask = np.zeros((ending_depth-start_z, h, w))\n",
    "        print(\"Created full img & mask\", full_volume_img.shape, full_volume_mask.shape)\n",
    "    full_volume_img[i] = tmp_img\n",
    "    full_volume_mask[i] = tmp_mask\n",
    "    i+=1\n",
    "    print(f\"done {z} z-slice\")\n",
    "print(\"full volume shape:\", full_volume_img.shape, full_volume_mask.shape)\n",
    "\n",
    "start_z = 0\n",
    "# ending_depth = full_volume_img.shape[0]\n",
    "if args.use_full_volume:\n",
    "    ending_height = full_volume_img.shape[1]\n",
    "    ending_width = full_volume_img.shape[2]\n",
    "print(\"subvolume shape:\", subvol_depth, subvol_height, subvol_width)\n",
    "print(\"ending:\", ending_depth, ending_height, ending_width)\n",
    "\n",
    "step_z = args.step_z\n",
    "step_y = args.step_y\n",
    "step_x = args.step_x\n",
    "while start_z < ending_depth:\n",
    "    end_z = start_z + subvol_depth\n",
    "    start_y = args.start_y\n",
    "    while start_y < ending_height:\n",
    "        end_y = start_y + subvol_height \n",
    "        start_x = args.start_x\n",
    "        while start_x < ending_width:\n",
    "            end_x = start_x + subvol_width\n",
    "            sub_volume_img = full_volume_img[start_z:end_z, start_y:end_y, start_x:end_x]\n",
    "            sub_volume_mask = full_volume_mask[start_z:end_z, start_y:end_y, start_x:end_x] # with confidence levels\n",
    "            sub_vol_depth, sub_vol_height, sub_vol_width = sub_volume_img.shape\n",
    "            image = torch.tensor(sub_volume_img).float().unsqueeze(0)\n",
    "            if (sub_vol_height < subvol_height) or (sub_vol_width < subvol_width) or (sub_vol_depth < subvol_depth):\n",
    "                image = tio.CropOrPad((subvol_depth, subvol_height, subvol_width))(image)\n",
    "                \n",
    "            try:\n",
    "                image = tio.ZNormalization()(image)\n",
    "                print(image.shape)\n",
    "                intermed_pred, sub_volume_pred = model(image)\n",
    "                binary_pred = torch.argmax(sub_volume_pred[0], dim=0) # (depth, height, width)\n",
    "                np.save(os.path.join(save_dir, \"original\", f\"z{start_z}_y{start_y}_x{start_x}.npy\"), sub_volume_img)\n",
    "                np.save(os.path.join(save_dir, \"ground_truth\", f\"z{start_z}_y{start_y}_x{start_x}.npy\"), sub_volume_mask)\n",
    "                np.save(os.path.join(save_dir, \"pred\", f\"z{start_z}_y{start_y}_x{start_x}.npy\"), sub_volume_pred.detach().cpu())            \n",
    "                fig, ax = plt.subplots(3, subvol_depth, figsize=(15,5), num=1)\n",
    "                visualize_3d_slice(sub_volume_img, ax[0], \"Image\")\n",
    "                visualize_3d_slice(sub_volume_mask, ax[1], \"Mask\")\n",
    "                visualize_3d_slice(binary_pred, ax[2], \"Pred\")\n",
    "                plt.savefig(os.path.join(save_dir, \"visualize\", f\"z{start_z}_y{start_y}_x{start_x}.png\"))\n",
    "                plt.close(\"all\")\n",
    "                print(f\"Saved z{start_z}-{end_z} y{start_y}-{end_y} x{start_x}-{end_x} subvolume\")\n",
    "            except:\n",
    "                print(f\"Skipping z{start_z}-{end_z} y{start_y}-{end_y} x{start_x}-{end_x}\")\n",
    "            start_x = start_x + step_x\n",
    "        start_y = start_y + step_y\n",
    "    start_z = start_z + step_z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gapvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
